<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>强化学习笔记</title>
    <link href="/2020/08/20/RLbook_001/"/>
    <url>/2020/08/20/RLbook_001/</url>
    
    <content type="html"><![CDATA[<blockquote><p><em>Reinforcement Learning An Introduction</em> - Richard Sutton &amp; Andrew Barto</p></blockquote><h1 id="k-臂老虎机（A-k-armed-Bandit-Problem）"><a href="#k-臂老虎机（A-k-armed-Bandit-Problem）" class="headerlink" title="$k$臂老虎机（A $k$-armed Bandit Problem）"></a>$k$臂老虎机（A $k$-armed Bandit Problem）</h1><p>$k$臂老虎机问题中，选择每个动作都会有相应的期望收益，称为这个动作的价值（value）。<br>记时刻$t$的动作为$A<em>t$，相应的收益为$R_t$，任意动作$a$的价值为$q</em>*(a)$，那么选择$a$时的期望收益为</p><script type="math/tex; mode=display">q_*(a) = \mathbb{E} \left[R_t | A_t = a\right]</script><p>现实中，我们不知道实际的动作价值，否则只要每次挑价值最高的动作就可以了。但是，我们可以估计这些动作的价值，记为$Q_t(a)$。</p><p>如果每次决策时，我们都选择估计价值最大的动作，则称这是贪婪的（greedy）。贪婪策略在短期内可以获得较大的收益，但长期看来这常常是短视的。因此有必要适时地选取一些非贪婪的动作。上述的贪婪行为又称为exploitation，而非贪婪行为又称为exploration。为了能够进行最优的决策，必须对二者进行权衡。</p><h1 id="动作价值（Action-value-Methods）"><a href="#动作价值（Action-value-Methods）" class="headerlink" title="动作价值（Action-value Methods）"></a>动作价值（Action-value Methods）</h1><p>动作的价值是选择它获取的平均回报，因此要估计一个动作的价值，自然可以对历史收益求平均，</p><script type="math/tex; mode=display">Q_t(a) = \frac{t时刻前动作a的总收益}{t时刻前选择动作a的总次数} =\frac{\sum_{i=1}^{t-1} R_i \cdot \mathbb{I}_{A_i=a}}{\sum_{i=1}^{t-1} \mathbb{I}_{A_i=a}}</script><p>根据大数定律，<script type="math/tex">Q_t (a)</script>收敛于<script type="math/tex">q_*(a)</script>。</p><p><strong>贪婪行为</strong>：</p><script type="math/tex; mode=display">A_t = \underset{a}{\text{arg max}} Q_t(a)</script><p><strong>$\varepsilon$-<em>greedy</em></strong>：大多数时候选取贪婪动作，每隔一段时间，例如以概率$\varepsilon$，等概率地随机从所有动作中选择一个，不考虑估计价值。</p><p>$\varepsilon$-<em>greedy</em>的优势在于，当采样次数趋近于无穷时，可以确保<script type="math/tex">Q_t (a)</script>收敛到<script type="math/tex">q_*(a)</script>。</p><blockquote><p><strong>Exercise 2.1</strong><br>In $\varepsilon$-<em>greedy</em> action selection, for the case of two actions and $\varepsilon = 0.5$, what is the probability that the greedy action is selected?</p></blockquote><p><strong>解答：</strong></p><script type="math/tex; mode=display">\begin{align}P(\text{select greedy}) = &P(\text{select greedy}|\text{exploitation}) P(\text{exploitation}) + \\ &P(\text{select greedy}|\text{exploration}) P(\text{exploration}) \\=&1 \cdot (1 - \varepsilon) + \frac{1}{k} \varepsilon \\=&1 - \frac{k-1}{k} \varepsilon\end{align}</script><p>需要注意即使是在exploration步骤，由于是随机选择动作，因此仍然有可能选择到贪婪的动作。</p><h1 id="10臂老虎机"><a href="#10臂老虎机" class="headerlink" title="10臂老虎机"></a>10臂老虎机</h1><p>一个小例子🌰，包含2000个随机生成的10臂老虎机问题，对于每个老虎机，动作的价值从一个标准正态分布中采样获得。在每个老虎机的实验中，运行1000个时间步，重复2000次实验，可以获知算法的平均性能。动作的价值则使用最简单的采样平均法）（初始估计值为0）。<br>根据实验结果，在较早的时间步，贪婪方法的平均收益提升较快，但很快就进入平台期；而非贪婪方法则能够继续提升，平均收益逐渐显著超过贪婪方法。</p><p>$\varepsilon$-<em>greedy</em>的优势也与具体任务有关，如果收益值的噪声较大，就需要更多的exploration才能获取最优动作，$\varepsilon$-<em>greedy</em>的优势也更大；如果收益值没有噪声，那么贪婪方法很快就能找到最优动作。如果任务是非平稳的，也就是动作价值会随时间发生变化，那么exploration就是一个必要的操作。</p><blockquote><p><strong>Exercise 2.2</strong><br>Consider a $k$-armed bandit problem with $k = 4$ actions, denoted 1, 2, 3, and 4. Consider applying to this problem a bandit algorithm using $\varepsilon$-greedy action selection, sample-average action-value estimates, and initial estimates of $Q_1(a) = 0$, for all $a$. Suppose the initial sequence of actions and rewards is $A_1 = 1, R_1 = -1, A_2 = 2, R_2 = 1, A_3 = 2, R_3 = -2, A_4 = 2, R_4 = 2, A_5 = 3, R_5 = 0$. On some of these time steps the $\varepsilon$ case may have occurred, causing an action to be selected at random. On which time steps did this definitely occur? On which time steps could this possibly have occurred?</p></blockquote><p><strong>解答：</strong></p><div class="table-container"><table><thead><tr><th>时间步</th><th>最优动作</th><th>动作</th></tr></thead><tbody><tr><td>1</td><td>1,2,3,4</td><td>1</td></tr><tr><td>2</td><td>2,3,4</td><td>2</td></tr><tr><td>3</td><td>2</td><td>2</td></tr><tr><td>4</td><td>3,4</td><td>2</td></tr><tr><td>5</td><td>2</td><td>3</td></tr></tbody></table></div><p>上表第二列是每个时间步决策前，估计价值最高的动作集合。<br>如果选择的动作不在最优动作集合中，那么必要发生了exploration。<br>因此，第4、5步必然是exploration，但其它也都有可能是exploration。</p><blockquote><p><strong>Exercise 2.3</strong><br>In the comparison shown in Figure 2.2, which method will perform best in the long run in terms of cumulative reward and probability of selecting the best action? How much better will it be? Express your answer quantitatively.<br>data:image/png;base64,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" data-original="" srcset="/img/loading.gif<img src="" alt=""></p></blockquote><p><strong>解答：</strong><br>$\varepsilon=0.01$的方法会取得最大的收益。</p>]]></content>
    
    
    
    <tags>
      
      <tag>reinforcement learning, machine learning, note</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>中文测试</title>
    <link href="/2020/07/17/test-math/"/>
    <url>/2020/07/17/test-math/</url>
    
    <content type="html"><![CDATA[<script type="math/tex; mode=display">a+b=c</script><blockquote><p>滚滚长江东逝水，浪花淘尽英雄。</p></blockquote><p><strong>这也是</strong>神奇的<em>一天</em>呢。</p>]]></content>
    
    
    
    <tags>
      
      <tag>test</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>文章测试</title>
    <link href="/2020/07/09/test-article/"/>
    <url>/2020/07/09/test-article/</url>
    
    <content type="html"><![CDATA[<h1 id="Get-start"><a href="#Get-start" class="headerlink" title="Get start!!!"></a>Get start!!!</h1><p>为什么不显示呢？<br>真奇怪。</p><p>Balalalallalalalalalala.</p><h1 id="First"><a href="#First" class="headerlink" title="First"></a>First</h1><p>This is a <strong>serious</strong> problem.</p><blockquote><p>Hand in hand.</p></blockquote><ul><li>Mango</li><li>Water</li><li><em>Banana</em></li></ul><h1 id="Start-again"><a href="#Start-again" class="headerlink" title="Start again"></a>Start again</h1><h2 id="Warm-up"><a href="#Warm-up" class="headerlink" title="Warm up"></a>Warm up</h2><p>$\mathop{x} \limits_a^b$</p><script type="math/tex; mode=display">\int_{i=1}^n x^2 \text{d}x</script><script type="math/tex; mode=display">\int_{i=1}^n x^2 \text{d}x</script><script type="math/tex; mode=display">\begin{equation}\int_{i=1}^n x^2 \text{d}x\end{equation}</script><script type="math/tex; mode=display">e_r(x)=\frac{x-x}{x^{*}}</script><script type="math/tex; mode=display">\min_{\mathbf{w},b} \frac{1}{2} \Vert \mathbf{w} \Vert^2 \quad s.t. \quad y_i(\mathbf{w}^T\phi(\mathbf{x})+b) \geq 1, \quad  i=1,2,...,m\qquad(9)</script><script type="math/tex; mode=display">\underset{a}{\text{arg max}}</script><script type="math/tex; mode=display">e\left ( x^{*} \right ) = x - x^{*}x = a_0 + \frac{1}{a_1 +\sqrt{a^2+b^2} \frac{1}{a_2 + \frac{1}{a_3 + a_4}}}\sqrt{a^2+b^2}</script><script type="math/tex; mode=display">f_n=f_{n-1}+f_{n-2}</script><script type="math/tex; mode=display">f_n</script><script type="math/tex; mode=display">f_1</script><script type="math/tex; mode=display">f^n</script><script type="math/tex; mode=display">f^1</script><script type="math/tex; mode=display">f</script><p>Then,</p><script type="math/tex; mode=display">\begin{aligned}a + b &= c\\\mathbf{x} &\sim \mathcal{N}(0, 1)\end{aligned}</script><script type="math/tex; mode=display">a + b = c</script><p>$a + b = c$</p><p>This equation <script type="math/tex">\cos 2\theta = \cos^2 \theta - \sin^2 \theta =  2 \cos^2 \theta - 1</script> is inline.</p><script type="math/tex; mode=display">\begin{aligned}\dot{x} & = \sigma(y-x) \\\dot{y} & = \rho x - y - xz \\\dot{z} & = -\beta z + xy\end{aligned}</script><script type="math/tex; mode=display">\begin{aligned}\dot{x} & = \sigma(y-x) \\\dot{y} & = \rho x - y - xz \\\dot{z} & = -\beta z + xy\end{aligned}</script><h2 id="Keyboard"><a href="#Keyboard" class="headerlink" title="Keyboard"></a>Keyboard</h2><pre><code class="hljs python"><span class="hljs-keyword">import</span> python <span class="hljs-keyword">as</span> pd<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> npdf = pd.DataFrame(np.random.randint(<span class="hljs-number">0</span>, <span class="hljs-number">100</span>, (<span class="hljs-number">50</span>, <span class="hljs-number">3</span>)))print(df)</code></pre>]]></content>
    
    
    
    <tags>
      
      <tag>test</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Hello World</title>
    <link href="/2020/07/09/hello-world/"/>
    <url>/2020/07/09/hello-world/</url>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><pre><code class="hljs bash">$ hexo new <span class="hljs-string">"My New Post"</span></code></pre><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><pre><code class="hljs bash">$ hexo server</code></pre><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><pre><code class="hljs bash">$ hexo generate</code></pre><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><pre><code class="hljs bash">$ hexo deploy</code></pre><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
